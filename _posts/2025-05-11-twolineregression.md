---
permalink: /2025/05/11/twolineregression
title: title tbd
subtitle: Linear Regression with two lines
date: 2025-05-11 19:38:14 +0530
layout: default
keywords: regression, neural networks, backprop
categories: nn
published: false
---

Let us sample some points from a line $L:y=mx+c$ and then attempt to fit a line $L_1:y=m_1x+c_1$ to the data points. Using an iterative algorithm like stochastic gradient descent guarantees convergence of the parameters $m_1, c_1$ to the true slope and intercept, $m, c.$ We don't add any noise anywhere and this is as simple as things get when one starts out in deep learning.


